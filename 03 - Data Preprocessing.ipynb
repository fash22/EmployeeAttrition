{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Phase 3: Data Preprocessing\n",
    "In this stage, the dataset will be formatted so that it can be used in machine learning model. That is, all data should be encoded numerically. At this stage, one-hot encoding and ordinal encoding will be used to encode non-numeric data. In this notebook, we will also execute the feature selection phase."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Cleaned Dataset\n",
    "Now, let's start by loading our cleaned dataset into a `Pandas.DataFrame` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Feature Names, convert them to numpy and them flatten them into one-dimension.\n",
    "ATTRS_NUM = pd.read_csv(\"dataset/constants/ATTRS_NUM.csv\", index_col=0).to_numpy().flatten()\n",
    "\n",
    "DATASET = pd.read_csv(\"dataset/cleaned/Dataset.csv\", index_col=\"EmployeeNumber\")\n",
    "DATASET.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Feature Matrix and Target Vector to be inputted on machine learning models.\n",
    "X = DATASET.iloc[:, 0:-1]\n",
    "y = DATASET.iloc[:, -1]\n",
    "\n",
    "# Conduct One-Hot Encoding on the Nominal Data of the feature matrix\n",
    "X = pd.get_dummies(data=X, drop_first=True)\n",
    "\n",
    "# Save the column names of the newly encoded dataset\n",
    "ATTRS_ENCODED = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert the X, and y DataFrames into NDArray\n",
    "X = X.values\n",
    "y = y.values\n",
    "\n",
    "# Target columns to be scaled\n",
    "COLUMNS_TO_BE_SCALED = [i for i in range(0,len(ATTRS_NUM))]\n",
    "\n",
    "# Define the column transformer with standard scaler targetted to columns defined in COLUMNS_TO_BE_SCALED.\n",
    "column_transformer = ColumnTransformer(\n",
    "  [(\"Standard Scaler\", StandardScaler(),  COLUMNS_TO_BE_SCALED),],\n",
    "  remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "# Scale the whole feature set\n",
    "X_scaled = column_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Fit the classifier to all of the dataset\n",
    "classifier = ExtraTreesClassifier(n_estimators=50)\n",
    "classifier.fit(X_scaled, y)\n",
    "\n",
    "# Select only the important features\n",
    "model = SelectFromModel(classifier, prefit=True)\n",
    "\n",
    "\n",
    "X_scaled_feature_selected = model.transform(X_scaled)\n",
    "\n",
    "# Save the names of the selected features\n",
    "model.feature_names_in_ = ATTRS_ENCODED\n",
    "ATTRS_SELECTED = model.get_feature_names_out()\n",
    "\n",
    "# Preview the selected features\n",
    "pd.DataFrame({\n",
    "  \"Selected Features\": ATTRS_SELECTED\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing the Dataset using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=22)\n",
    "\n",
    "X_scaled_resampled, y_resampled = smote.fit_resample(X_scaled_feature_selected, y)\n",
    "# X_scaled_resampled, y_resampled = smote.fit_resample(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the features and targets into a training set and test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled_resampled, y_resampled, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the training and testing set.\n",
    "pd.DataFrame(X_train).to_csv(\"dataset/preprocessed/Features_Training_Set.csv\")\n",
    "pd.DataFrame(X_test).to_csv(\"dataset/preprocessed/Features_Testing_Set.csv\")\n",
    "pd.Series(y_train).to_csv(\"dataset/preprocessed/Target_Training_Set.csv\")\n",
    "pd.Series(y_test).to_csv(\"dataset/preprocessed/Target_Testing_Set.csv\")\n",
    "\n",
    "# Export the names of the selected features\n",
    "pd.Series(ATTRS_SELECTED).to_csv(\"./dataset/constants/ATTRS_SELECTED.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e60729ca8ea56617ec8adb4fdd934c62df498378cebc3e80f6970f4b45575174"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
